Autoregressive [[Generative Models]] are models that generate data by recursively applying a fixed function to the output of the previous step. In the case of language modeling, an autoregressive model would generate one word at a time by conditioning each word on the previous words in the sequence. The output at each time step is generated using a learned probability distribution, typically a softmax distribution over a fixed vocabulary. Examples of autoregressive models include the recurrent neural network language model (RNNLM) and the transformer language model.